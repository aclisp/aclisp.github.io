---
layout: post
title:  "Docker 在生产环境的实践"
date:   2016-05-07
categories: blog
---

从去年到现在，我们逐渐在生产环境中应用了 docker，支撑了云数据库和分布式数据库两个上层系统。

在实践中，我们把平台和流程提取出来，形成了一套管理工具，也就是 —— Sigma。它的特性有：

* TOC
{:toc}

Sigma 的宗旨是，**重用（reuse）**和**通用** —— 基于 Google 开源的 kubernetes 实现，让系统有了生命力；基于 docker 容器技术，让多样化部署成为可能。

# 统合资源再进行细粒度分配

就是把物理服务器的计算资源统合起来。每个服务器有：

* 地域位置（所在机房，所属网络运营商）
* 容量（CPU、内存、硬盘）
* 以及一些状态（正常／异常，实时利用率）

统合之后，就是一个池子，也可以看成是**一个超级服务器**，总容量是所有计算资源的累加（Sigma 的数学符号 Σ 意思就是「累加」）。

业务要用，就从池子里分配。分配的粒度很自由，从 0 到单个服务器最大容量之间的任何值都可以。

分配完毕，就要考虑调度问题。也就是，具体在哪个服务器上运行业务。

# 基于标签的自动化调度

调度，就是考虑剩余可用资源、均匀化分布等因素，把业务部署到合适的服务器上运行。此外，我们考虑到业务之间的逻辑关系，会反映到部署位置上，还做了基于标签的调度策略。

我们给每个服务器打上标签，业务也有对应的标签。只有匹配一致，才计算剩余容量和分布密度，决定最终的部署位置。

这样做的好处是，灵活，也满足机房亲近性的需求。

# 业务实例隔离的手段

在考量每个业务实例的分配策略时，我们发现，**业务分配的容量不代表业务的容量上限**。

举例来说，业务要求分配 `1核/2G`，实际运行上限是 `4核/8G`。分配完毕之后，平台从池子剩余容量里减去 `1核/2G`！（而不是 `4核/8G`）。基于这个思路，可以实现**超配**。

容量上限是用 cgroups 保证（docker 自带的特性）。如果业务实例出现问题（CPU 100%，内存泄漏等），不会把整个服务器的资源耗尽，也不会影响到这个服务器上的其他实例。

一个有趣的观察是，业务在初期通常不知道自己要分配多少，上限写多少。只有上线运行一段时间，收集了指标反馈，估算了增长才能确定。针对这种情况，我们改写了 kubernetes 的代码，让容量随时可调。业务初期上限可以设 0，就是不限制。

# 业务实例迁移的流程

我们的 docker 容器集群使用 flannel 做覆盖网络。每个业务实例就是一个容器，像虚拟机一样，有自己的 IP，用于跨机房通信。

一台物理服务器故障，需要新找一台顶上。这时，原来的业务实例都要迁移到新服务器上。由于容器 IP 是虚拟的，迁移之后不变。这种特性对于业务很友好。

为了实现这个特性，我们修改了 kubernetes 的代码，在创建 docker 容器时，指定一个由系统分配的 IP。

目前平滑迁移流程还需要一些繁琐的管理操作。在新版本里会逐步自动化。

# 解决跨机房容器互联

我们在实践中对 docker 容器互联总结了两种有效的网络模型：

* 覆盖网络
* 主机兼容网络

它们都不需要改动服务器和交换机。

覆盖网络前面说过了。分布式数据库业务对网络性能的要求比较高。为此，我们专门设计了主机兼容网络。

每个业务实例除了有 `容器IP:Port`，还有系统分配的 `主机IP:Port`。后者通过环境变量注入到容器内，业务有需要时读取。业务实例自组集群时，如果用的是 `主机IP:Port`，则跳过了覆盖网络的开销。实测性能几乎无损耗。

主机兼容网络作为备选方案，对业务有侵入，也失去一定的灵活性；但获得了可靠的性能。另外，覆盖网络还有很大的优化空间：临近机房部署、智能选路等，有很多文章可以做。

| 模型       |  通信方式            | 业务侵入性        | 灵活性          | 性能 | 适用场景(注2)  |
| ---------- | --------------     | --------------   | -----         | ---- | -------- |
| 覆盖网络    | 容器IP＋业务Port     | 无。可从 eth0 获取 | 高。迁移保持不变 | 有损耗 | Web, ZooKeeper |
| 主机兼容网络 | 主机IP＋业务Port (**注1**)| 有。可从环境变量获取| 低。依赖主机IP  | 无损耗 | MySQL, MongoDB |

* **注1** 由系统分配的随机端口
* **注2** 指在 Sigma 上部署的业务集群种类

上表列出了不同网络模型的优劣，供业务取舍和采纳。

值得注意的是，不管用哪种网络模型，容器的网络都与主机网络是隔离的：**解决了业务一机多装（多个副本）的需求，没有 IP 地址和端口冲突的困扰**。

# 支持 SSD 存储卷插件

一般 Web 类业务通常是无状态的。但 MySQL，NoSQL 类业务产生的数据有本地存储的需求。为此，我们开发了 kubernetes 存储卷插件，可以挂载主机上的空闲 SSD 硬盘到容器的文件系统。

开发这个特性，是为了满足业务的高性能存储需求，比如写日志、写表等。但数据的高可用，还要业务自己去做。

# 从业务发布到容器化部署

正在开发中的发布系统和控制台可以提升用户体验，做到资源可视化。

# 配置中心和服务发现

Sigma 从平台的层面实现了配置中心。业务＝程序包＋配置＋启动脚本，docker image 就是程序包，配置和启动脚本从中心下发。

Sigma 也从平台的层面实现了服务发现。如果业务需要自组集群，可以通过 API 接口获取集群中其他实例的 IP 地址。

基于这两个特性，我们在 Sigma 上成功部署多个 Hadoop 集群，验证了对新业务的支撑能力。

# Kubernetes 的修改汇总

我们从 kubernetes 1.1 fork 分支，已经在上面做了 30 多次提交。大部分是为了适配业务需求和部署环境，有几个是 bug fix，用 issue 和 PR 反馈回社区。一些关键的修改如下：

* Host port allocation during container creation
* Volume plugin for SSD allocation
* Enable Pod spec live update
* Disable container using swap memory
* Predictable container startup sequence
* Precise container-killed reason
* Put Pod in NeedMigration state when Node is broken
* IP allocation during container creation

最近计划升级到 kubernetes 1.2。

关于生命力，为了[保持开发分支与上游同步](https://github.com/kubernetes/kubernetes/blob/master/docs/devel/development.md#keeping-your-development-fork-in-sync)，我们用如下命令把社区最新的特性进展和问题修复纳入自己的代码：

    git remote add upstream 'https://github.com/kubernetes/kubernetes.git'

    git fetch upstream
    git rebase upstream/release-1.1

