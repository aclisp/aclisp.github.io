---
layout: post
title:  "Sigma 系统"
date:   2016-04-28
categories: blog
---

我们在生产环境实践 docker 的过程中，不但形成了一个最佳的套路，还逐步提炼了最贴近业务的需求。分项来说，就是：

* 从业务发布到容器化部署的方法
* 结合 docker 和 kubernetes 改写适应特殊环境的实践
* 统合资源再进行细粒度分配的框架
* 基于 cgroups 的业务实例隔离手段
* 基于 vxlan 的业务实例平滑迁移流程
* TODO 基于 ConfigMap 的通用配置中心实现

这就是所谓的 Sigma 系统，也是我们从去年到现在的成果。

值得一提的是，公司内部兄弟部门也有一些相似的想法和产品出来。例如数据部的 “Hadoop 多租户平台”和运维部的 “NoSQL 资源管理及调度平台”。

我们的 Sigma 与它们的区别在于，**重用**和**通用**——基于 Google 开源的 kubernetes 实现，让系统有了生命力；基于 docker 容器技术，让多样化部署成为可能。

# 统合资源再进行细粒度分配

就是把物理服务器的计算资源统合起来。每台物理服务器有如下特性：

* 地域位置（所在机房，所属网络运营商）
* 容量（CPU、内存、硬盘）
* 以及一些状态（正常／异常，实时利用率）

统合之后，就是一个池子，也可以看成是**一个**超级服务器，总容量是所有计算资源的累加。（Sigma 的数学符号意思就是“累加”）

业务要用，就从池子里再分配。分配的粒度很自由，从 0 到单台服务器最大容量之间的任何值都可以。0 就是 Best-Effort.

分配完毕，就要考虑调度问题。也就是，具体在哪个物理服务器上运行业务。

# 基于标签的调度策略

一般的调度策略，都考虑了剩余可用资源；均匀化分布等因素。除此之外，我们考虑到了业务之间的逻辑关系，会反映到部署位置上，特别做了基于标签的调度策略。

我们给每个物理服务器打上标签，业务也有对应的标签。只有匹配一致，才决定最终的部署位置。

好处是，灵活、也满足机房亲近性的需求。

# 基于 cgroups 的业务实例隔离手段

首先，一个重要的思想就是，**业务分配的容量不代表业务的容量上限**。这个思想的提出，仍然是为了灵活！

举例来说，业务要求分配 1C/2G ,实际运行上限 4C/8G。分配完毕之后，平台从池子可用容量里减去 1C/2G!（而不是 4C/8G）基于这个思路，可以实现**超配**。

容量上限就是用 cgroups 保证（docker 自带）。

一个有趣的观察是，业务在初期通常不知道自己要分配多少，上限写多少。只有上线运行一段时间，收集了指标反馈，估算了增长才能确定。针对这种情况，我们改写了 kubernetes 的代码，让容量随时可调。业务初期上限可以设 0，就是不限制。

# 基于 vxlan 的业务实例平滑迁移流程

我们的 docker 容器集群使用 flannel （底层可以选用 vxlan 或 udp 隧道）做覆盖网络。每个容器都有自己的 IP，可用于跨机房通信。

一台物理服务器故障，需要新找一台顶上。这时，原来的业务实例都要迁移到新的服务器上。由于容器 IP 是虚拟的，迁移之后不变。这种特性对于业务很友好。

原生 kubernetes 不支持固定的容器 IP。我们修改了 kubernetes 的代码，在创建 docker 容器时，去指定一个由系统分配的 IP。

目前平滑迁移流程还需要一些繁琐的管理操作。在新版本里会逐步自动化。

# 覆盖网络的性能问题

我们在实践中对 docker 容器集群总结出了两种有效的网络模型：

* 覆盖网络
* 主机兼容网络

覆盖网络在性能和稳定性上还不够成熟，而分布式数据库业务对网络性能的要求比较高。为此，我们专门设计了主机兼容网络。

每个业务实例除了有 “容器IP:Port” ，还有系统分配的 “主机IP:Port”。后者通过环境变量注入到容器内，业务可以在有需要时读取。业务实例在自组集群时，如果用的是 “主机IP:Port”，则跳过了覆盖网络的开销。实测性能几乎无损耗。

主机兼容网络作为备选方案，对业务有侵入，也失去一定的灵活性；但获得了可靠的性能。另外，覆盖网络的性能还有很大优化的空间：临近机房部署、智能选路等，有很多文章可以做。

| 模型       |  通信方式            | 业务侵入性        | 灵活性          | 性能 | 适用场景(注2)  |
| ---------- | --------------     | --------------   | -----         | ---- | -------- |
| 覆盖网络    | 容器IP＋业务Port     | 无。可从 eth0 获取 | 高。迁移保持不变 | 有损耗 | Web, ZooKeeper |
| 主机兼容网络 | 主机IP＋业务Port (**注1**)| 有。可从环境变量获取| 低。依赖主机IP  | 无损耗 | MySQL, MongoDB |

* **注1** 由系统分配的随机端口
* **注2** 指在 Sigma 系统上部署的业务集群种类

上表列出了不同网络模型的优劣，供业务取舍和采纳。

值得注意的是，不管用哪种网络模型，容器的网络都与主机网络是隔离的：解决了业务一机多装（多个副本）的需求，没有 IP 地址和端口冲突的困扰。这与 `docker run --net=host` 完全不同。

# 支持 SSD 的存储卷插件

一般 Web 类业务通常是无状态的。但 MySQL, NoSQL类业务产生的数据有本地存储的需求。为此，我们开发了 kubernetes volume plugin，
可以指定挂载主机上的空闲 SSD 硬盘到容器的文件系统。

开发这个特性，主要是为了满足业务的高性能存储需求，比如写日志，写表等。但数据的高可用，业务必须自己去做。

# Kubernetes 的修改汇总

我们从 kubernetes 1.1 fork 分支，在上面做了 30 多次提交。大部分都是为了适配业务需求和部署环境的，有几个是 bug fix 则用 issue 和 PR
反馈回社区。一些关键的修改如下：

* Host port allocation during container creation
* Volume plugin for SSD allocation
* Enable Pod spec live update
* Disable container using swap memory
* Predictable container startup sequence
* Precise container-killed reason
* Put Pod in NeedMigration state when Node is broken
* IP allocation during container creation (WIP)

最近计划升级到 kubernetes 1.2。

关于生命力，为了[保持开发分支与上游同步](https://github.com/kubernetes/kubernetes/blob/master/docs/devel/development.md#keeping-your-development-fork-in-sync)，我们用如下命令把社区最新的特性进展和问题修复纳入自己的代码：

    git remote add upstream 'https://github.com/kubernetes/kubernetes.git'

    git fetch upstream
    git rebase upstream/release-1.1

# 从业务发布到容器化部署

正在开发中的发布系统和控制台可以提升用户体验和资源可视化。

# 基于 ConfigMap 的通用配置中心实现

TODO